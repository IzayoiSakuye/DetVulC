# gnn_vuln_scanner/multilabel_trainer.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import DataLoader
from sklearn.metrics import hamming_loss, jaccard_score, classification_report
import numpy as np
from tqdm import tqdm
import random
import json
import pickle


class MultiLabelGNNTrainer:
    """å¤šæ ‡ç­¾GNNè®­ç»ƒå™¨"""

    def __init__(self, model, device, num_classes=10, class_names=None):
        self.model = model.to(device)
        self.device = device
        self.num_classes = num_classes
        self.class_names = class_names or [f'class_{i}' for i in range(num_classes)]

        # ä½¿ç”¨BCEWithLogitsLosså¤„ç†å¤šæ ‡ç­¾åˆ†ç±»
        self.criterion = nn.BCEWithLogitsLoss()
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)

    def train_epoch(self, train_loader):
        self.model.train()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0

        progress_bar = tqdm(train_loader, desc="Training")
        for data in progress_bar:
            data = data.to(self.device)

            # ç¡®ä¿æ ‡ç­¾æ˜¯floatç±»å‹
            targets = data.y.float()

            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆé˜ˆå€¼ä¸º0.5ï¼‰
            predicted = (torch.sigmoid(outputs) > 0.5).float()
            correct_predictions += (predicted == targets).sum().item()
            total_predictions += targets.numel()

            accuracy = 100. * correct_predictions / total_predictions
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{accuracy:.2f}%'
            })

        return total_loss / len(train_loader), accuracy

    def validate(self, val_loader):
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        all_probabilities = []

        with torch.no_grad():
            for data in tqdm(val_loader, desc="Validating"):
                data = data.to(self.device)
                targets = data.y.float()

                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
                total_loss += loss.item()

                # æ”¶é›†é¢„æµ‹ç»“æœ
                probabilities = torch.sigmoid(outputs)
                predicted = (probabilities > 0.5).float()
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())

        if not all_predictions:
            return 0, 0, [], [], []

        avg_loss = total_loss / len(val_loader)
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        all_probabilities = np.array(all_probabilities)

        # è®¡ç®—å„ç§æŒ‡æ ‡
        hamming = hamming_loss(all_targets, all_predictions)
        try:
            jaccard = jaccard_score(all_targets, all_predictions, average='samples')
        except:
            jaccard = 0

        accuracy = 100. * (1 - hamming)

        return avg_loss, accuracy, all_predictions, all_targets, all_probabilities

    def train(self, train_loader, val_loader, epochs=30):
        best_acc = 0
        train_losses, val_losses = [], []
        train_accs, val_accs = [], []

        for epoch in range(epochs):
            print(f'\nEpoch {epoch + 1}/{epochs}')
            print('-' * 50)

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            train_losses.append(train_loss)
            train_accs.append(train_acc)

            # éªŒè¯
            val_loss, val_acc, preds, targets, probs = self.validate(val_loader)
            val_losses.append(val_loss)
            val_accs.append(val_acc)

            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_acc:
                best_acc = val_acc
                torch.save(self.model.state_dict(), 'best_multilabel_gnn_model.pth')
                print(f'âœ… Best model saved with accuracy: {best_acc:.2f}%')

        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs,
            'best_acc': best_acc
        }


def split_dataset(graph_data_list, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """åˆ’åˆ†æ•°æ®é›†"""
    print("ğŸ“Š åˆ’åˆ†æ•°æ®é›†...")

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    random.seed(42)
    random.shuffle(graph_data_list)

    total_size = len(graph_data_list)
    train_size = int(total_size * train_ratio)
    val_size = int(total_size * val_ratio)

    train_data = graph_data_list[:train_size]
    val_data = graph_data_list[train_size:train_size + val_size]
    test_data = graph_data_list[train_size + val_size:]

    print(f"   è®­ç»ƒé›†: {len(train_data)} ä¸ªå›¾")
    print(f"   éªŒè¯é›†: {len(val_data)} ä¸ªå›¾")
    print(f"   æµ‹è¯•é›†: {len(test_data)} ä¸ªå›¾")

    return train_data, val_data, test_data


def load_graph_dataset(dataset_file):
    """åŠ è½½å›¾æ•°æ®é›†"""
    print(f"ğŸ“‚ åŠ è½½å›¾æ•°æ®é›†: {dataset_file}")

    with open(dataset_file, 'rb') as f:
        graph_data_list = pickle.load(f)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(graph_data_list)} ä¸ªå›¾")

    # éªŒè¯æ ‡ç­¾ç»´åº¦
    valid_data_list = []
    invalid_count = 0

    for data in graph_data_list:
        try:
            if hasattr(data, 'y') and len(data.y) == 10:  # 10ä¸ªç±»åˆ«
                valid_data_list.append(data)
            else:
                invalid_count += 1
        except Exception as e:
            print(f"âš ï¸  ç§»é™¤æ— æ•ˆæ•°æ®: {e}")
            invalid_count += 1

    print(f"âœ… æœ‰æ•ˆæ•°æ®: {len(valid_data_list)} ä¸ªå›¾ (ç§»é™¤æ— æ•ˆæ•°æ®: {invalid_count})")
    return valid_data_list


def analyze_multilabel_results(predictions, targets, class_names):
    """åˆ†æå¤šæ ‡ç­¾åˆ†ç±»ç»“æœ"""
    print("\nğŸ“‹ å¤šæ ‡ç­¾åˆ†ç±»è¯¦ç»†æŠ¥å‘Š:")

    # æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    from sklearn.metrics import precision_recall_fscore_support

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    precision, recall, f1, support = precision_recall_fscore_support(
        targets, predictions, average=None, zero_division=0
    )

    print("ğŸ“Š å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡:")
    print(f"{'ç±»åˆ«':<20} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'æ”¯æŒæ•°':<10}")
    print("-" * 70)
    for i, class_name in enumerate(class_names):
        print(f"{class_name:<20} {precision[i]:<10.3f} {recall[i]:<10.3f} {f1[i]:<10.3f} {int(support[i]):<10}")

    # æ•´ä½“æŒ‡æ ‡
    hamming = hamming_loss(targets, predictions)
    try:
        subset_accuracy = np.mean(np.all(predictions == targets, axis=1))
        jaccard = jaccard_score(targets, predictions, average='samples')
    except:
        subset_accuracy = 0
        jaccard = 0

    print(f"\nğŸ¯ æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
    print(f"   Hamming Loss: {hamming:.4f}")
    print(f"   Subset Accuracy: {subset_accuracy:.4f}")
    print(f"   Jaccard Score: {jaccard:.4f}")


def main_train_multilabel_model():
    """è®­ç»ƒå¤šæ ‡ç­¾æ¨¡å‹çš„ä¸»å‡½æ•°"""
    dataset_file = "data/processed_output/multilabel_graph_dataset.pkl"
    vuln_types = [
        'buffer_overflow', 'use_after_free', 'double_free', 'null_pointer',
        'integer_overflow', 'format_string', 'command_injection',
        'path_traversal', 'race_condition', 'memory_leak'
    ]

    try:
        print("ğŸš€ å¼€å§‹å¤šæ ‡ç­¾GNNæ¼æ´æ£€æµ‹æ¨¡å‹è®­ç»ƒ...")
        print("=" * 60)

        # åŠ è½½å›¾æ•°æ®é›†
        graph_data_list = load_graph_dataset(dataset_file)

        if not graph_data_list:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®!")
            return None, None

        # åˆ’åˆ†æ•°æ®é›†
        train_data, val_data, test_data = split_dataset(graph_data_list)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        if len(train_data) == 0 or len(val_data) == 0:
            print("âŒ è®­ç»ƒé›†æˆ–éªŒè¯é›†ä¸ºç©º!")
            return None, None

        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

        # åˆå§‹åŒ–æ¨¡å‹
        model = MultiLabelVulnGNN(input_dim=13, hidden_dim=128, num_classes=10)
        print(f"ğŸ§  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

        # è®­ç»ƒæ¨¡å‹
        trainer = MultiLabelGNNTrainer(model, device, num_classes=10, class_names=vuln_types)
        results = trainer.train(train_loader, val_loader, epochs=30)

        print(f"\nğŸ† è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {results['best_acc']:.2f}%")

        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        print("\nğŸ§ª æµ‹è¯•é›†è¯„ä¼°...")
        test_loss, test_acc, test_preds, test_targets, test_probs = trainer.validate(test_loader)
        print(f"ğŸ æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.2f}%")

        # è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        analyze_multilabel_results(test_preds, test_targets, vuln_types)

        # ä¿å­˜ç»“æœ
        results['test_acc'] = test_acc
        with open('multilabel_training_results.json', 'w') as f:
            json.dump(results, f, indent=2)

        # ä¿å­˜æµ‹è¯•é¢„æµ‹ç»“æœ
        test_results = {
            'predictions': test_preds.tolist(),
            'targets': test_targets.tolist(),
            'probabilities': test_probs.tolist()
        }
        with open('multilabel_test_predictions.pkl', 'wb') as f:
            pickle.dump(test_results, f)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜!")
        print("   - æ¨¡å‹æƒé‡: best_multilabel_gnn_model.pth")
        print("   - è®­ç»ƒæ—¥å¿—: multilabel_training_results.json")
        print("   - æµ‹è¯•ç»“æœ: multilabel_test_predictions.pkl")

        return model, results

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None


if __name__ == "__main__":
    main_train_multilabel_model()
